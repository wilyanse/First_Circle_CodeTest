{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Governance\n",
    "A logger was implemented using `logger_config.py` in the base directory using the logging module and previous files were updated to implement the logger, specifically the `processor.py` and the `dataloading.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger setup\n",
    "The logger was setup to obtain the source of the logger call by having it be instantiated with a parameter `name`. The parameter is passed into different loggers but will be collated into one file, the `data_pipeline.log` file. The logger also sets the stream to only collect INFO level logs but the file to collect DEBUG level logs, supposedly lessening the stream outputs. The logs are formatted in order to get the time when the log was created, the source of the log in `name`, the urgency of the log in `level`, and the log message itself. The logger was then spread out into the `processor.py` file and the `dataloading.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler('data_pipeline.log')\n",
    "    \n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor logs\n",
    "The logs in `processor.py` handles mostly processing level data such as the number of records from each file, as well as catching the errors such as column inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger_config import setup_logger\n",
    "\n",
    "logger = setup_logger('processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger was first setup by importing `logger_config` and calling the `setup_logger` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_csv(self, filepath):\n",
    "    new_entries = pd.read_csv(filepath)\n",
    "    try:\n",
    "        new_entries = self.clean_data(new_entries)\n",
    "        new_entries = new_entries[self.columns]\n",
    "        logger.info('Adding ' + str(new_entries.shape[0]) + ' entries from: ' + filepath)\n",
    "        self.dataframe = (self.dataframe.copy() if new_entries.empty else new_entries.copy() if self.dataframe.empty\n",
    "                            else pd.concat([self.dataframe, new_entries], ignore_index=True)\n",
    "                            )\n",
    "    except KeyError as e:\n",
    "        logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is being read in `add_csv`, the logger takes note of the entries from the filepath and catches the error when a column is called that does not exist for the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entries(self, row):\n",
    "    errors = []\n",
    "    for cur in range(len(self.columns)):\n",
    "        if self.types[cur] == 'datetime':\n",
    "            if pd.isna(row[self.columns[cur]]):\n",
    "                errors.append(str(self.columns[cur]) + ' is not datetime')\n",
    "                logger.error(str(self.columns[cur]) + ' is not datetime')\n",
    "        elif self.types[cur] == 'int':\n",
    "            if not isinstance(row[self.columns[cur]], int):\n",
    "                if self.columns[cur] == -1:\n",
    "                    errors.append(str(self.columns[cur]) + ' has an incorrect data type')\n",
    "                    logger.error(str(self.columns[cur]) + ' has an incorrect data type')\n",
    "                    continue\n",
    "                errors.append(str(self.columns[cur]) + ' is not ' + str(self.types[cur]))\n",
    "                logger.error(str(self.columns[cur]) + ' is not ' + str(self.types[cur]))\n",
    "        else:\n",
    "            if not isinstance(row[self.columns[cur]], str):\n",
    "                errors.append(str(self.columns[cur]) + ' is not ' + str(self.types[cur]))\n",
    "                logger.error(str(self.columns[cur]) + ' is not ' + str(self.types[cur]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `check_entries` function which was the main error checking tool previously now adds the same error messages into the logger file instead of storing it into an array of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the records loaded into PostgreSQL\n",
    "We modify the code for `upload_data` as this was the only code we implemented to upload code to PostgreSQL to include a logger instead of printing to the output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data():\n",
    "    users = transformation.users\n",
    "    users.to_sql('users', engine, if_exists='replace', index=False)\n",
    "    logger.info(str(users.shape[0]) + ' entries were added to PostgreSQL.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
