{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "In this Python notebook, I explain the thought process behind implementing the Data Loading portion of the exam specifications. The data loading process itself is simply just the `dataloading.py` file but it invovles external applications such as Docker for accesibility on the user's side.\n",
    "\n",
    "Note that running this Python notebook may result into errors as the directories for this notebook and the original file for `dataloading.py` are different. To see the Data loading script, run `dataloading.py` from the original directory instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Docker container\n",
    "We start by creating a `docker-compose.yml` file that contains basic details regarding the PostgreSQL database that we will be instantiating and running. The file contains a basic template of most docker-compose files that involve creating a docker container that runs PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  db:\n",
    "    image: postgres:latest\n",
    "    environment:\n",
    "      POSTGRES_USER: myuser\n",
    "      POSTGRES_PASSWORD: mypassword\n",
    "      POSTGRES_DB: mydatabase\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - db_data:/var/lib/postgresql/data\n",
    "\n",
    "volumes:\n",
    "  db_data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a `Dockerfile` that automatically obtains the latest version of PostgreSQL as well as their initialization scripts. We can then simply run the following commands to get started with our Docker PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "-`docker pull postgres`\n",
    "\n",
    "-`docker run --name my_postgres -e POSTGRES_PASSWORD=mysecretpassword -d postgres`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PostgreSQL database is running in a Docker container, we can then proceed with loading the data into it with `dataloading.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Connection from Python\n",
    "We use the library for SQLAlchemy to connect with our PostgreSQL database, as well as easily create SQL queries with our Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "dbname=\"postgres\"\n",
    "user=\"postgres\"\n",
    "password=\"munch\"\n",
    "host=\"localhost\"\n",
    "port = '5432'\n",
    "\n",
    "connection_string = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the basic login prerequisites to connect to the database and have our engine running. Once it's ready, we can then use queries to load the dataframes we've been working with into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the dataframe\n",
    "We store the code for uploading the pandas dataframe in the `upload_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformation\n",
    "\n",
    "def upload_data():\n",
    "    users = transformation.users\n",
    "    users.to_sql('users', engine, if_exists='replace', index=False)\n",
    "    print(\"Data uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes the users table that we've transformed from the previous section and uploads it using the enginer we've made with SQLAlchemy. We then test this by looking at the query we receive from running the `query_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def query_data():\n",
    "    query = 'SELECT * FROM users'\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(\"Data fetched successfully!\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once run, the query selects all the data from the `users` table and stores it into a pandas dataframe. We notice that pandas automatically concatenates an index unto the dataframe, despite having removed it from the previous section. However, this shows that the data is indeed loaded into the database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
